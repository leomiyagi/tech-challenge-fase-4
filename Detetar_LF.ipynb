{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta criada: c:\\Users\\nando\\OneDrive\\Área de Trabalho\\Rec.facial\\TC4\\tech-challenge-fase-4\\detected_faces\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\nando/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\nando/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\nando/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\nando/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\nando/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "Pasta já existente: c:\\Users\\nando\\OneDrive\\Área de Trabalho\\Rec.facial\\TC4\\tech-challenge-fase-4\\detected_faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando vídeo:   7%|▋         | 225/3326 [01:52<25:56,  1.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 170\u001b[0m\n\u001b[0;32m    163\u001b[0m output_video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(script_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_video_11.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Nome do vídeo de saída\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Chamar a função para detectar emoções no vídeo e salvar o vídeo processado\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[43mdetect_emotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_video_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m, in \u001b[0;36mdetect_emotions\u001b[1;34m(video_path, output_path)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     51\u001b[0m rgb_frame \u001b[38;5;241m=\u001b[39m frame[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 53\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     56\u001b[0m     box \u001b[38;5;241m=\u001b[39m face\u001b[38;5;241m.\u001b[39mbbox\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\insightface\\app\\face_analysis.py:75\u001b[0m, in \u001b[0;36mFaceAnalysis.get\u001b[1;34m(self, img, max_num)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m taskname\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetection\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(face)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\insightface\\model_zoo\\attribute.py:83\u001b[0m, in \u001b[0;36mAttribute.get\u001b[1;34m(self, img, face)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#assert input_size==self.input_size\u001b[39;00m\n\u001b[0;32m     82\u001b[0m blob \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(aimg, \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_std, input_size, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_mean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_mean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_mean), swapRB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 83\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mblob\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaskname\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenderage\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pred)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:266\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    264\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "import uuid \n",
    "\n",
    "def detect_emotions(video_path, output_path):\n",
    "    # Capturar vídeo do arquivo especificado\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Verificar se o vídeo foi aberto corretamente\n",
    "    if not cap.isOpened():\n",
    "        print(\"Erro ao abrir o vídeo.\")\n",
    "        return\n",
    "\n",
    "    # Obter propriedades do vídeo\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Definir o codec e criar o objeto VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec para MP4\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    app = FaceAnalysis()\n",
    "    app.prepare(ctx_id=0)  # Use ctx_id=-1 for CPU\n",
    "    \n",
    "    diretorio_saida=\"detected_faces\"\n",
    "    \n",
    "    criar_pasta_para_rostos(diretorio=diretorio_saida)\n",
    "    \n",
    "   \n",
    "    Rastreador = {}\n",
    "    proximo_id = 1\n",
    "    Rastreados = set()\n",
    "\n",
    "    # Loop para processar cada frame do vídeo\n",
    "    for frame_idx in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n",
    "        # Ler um frame do vídeo\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Se não conseguiu ler o frame (final do vídeo), sair do loop\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = frame[:, :, ::-1]\n",
    "\n",
    "        faces = app.get(rgb_frame)\n",
    "\n",
    "        for face in faces:\n",
    "            box = face.bbox.astype(int)\n",
    "            \n",
    "            x1, y1, x2, y2=ajustar_limites_rosto(box, width, height)\n",
    "\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "\n",
    "            face_roi = rgb_frame[y1:y2, x1:x2]\n",
    "            \n",
    "            ja_existe = False\n",
    "            id_face = None\n",
    "            \n",
    "            if face_roi.size == 0:\n",
    "                print(f\"Rosto inválido detectado no frame {frame_idx}, ignorando.\")\n",
    "                continue\n",
    "            else:\n",
    "                face_id, proximo_id, Rastreador = rastrear_ou_criar_id(box, Rastreador, proximo_id)\n",
    "                Rastreador[face_id] = ((x1, y1, x2, y2), frame_idx)\n",
    "                salvar_rosto(face_roi, face_id, Rastreados, diretorio_saida)\n",
    "                     \n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "\n",
    "                analysis = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "                if isinstance(analysis, list) and len(analysis) > 0:\n",
    "                    dominant_emotion = analysis[0]['dominant_emotion']\n",
    "                else:\n",
    "                    dominant_emotion = \"Unknown\"\n",
    "\n",
    "            except Exception as e:\n",
    "                dominant_emotion = \"Error\"\n",
    "                print(f\"Error analyzing emotion: {e}\")\n",
    "\n",
    "            cv2.putText(frame, dominant_emotion, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "        # Escrever o frame processado no arquivo de vídeo de saída   \n",
    "        out.write(frame)\n",
    "\n",
    "    # Liberar a captura de vídeo e fechar todas as janelas\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def criar_pasta_para_rostos(diretorio=\"detected_faces\"):\n",
    "    # Obter o caminho absoluto\n",
    "    caminho_absoluto = os.path.abspath(diretorio)\n",
    "    \n",
    "    # Criar a pasta se ela não existir\n",
    "    if not os.path.exists(caminho_absoluto):\n",
    "        os.makedirs(caminho_absoluto)\n",
    "        print(f\"Pasta criada: {caminho_absoluto}\")\n",
    "    else:\n",
    "        print(f\"Pasta já existente: {caminho_absoluto}\")\n",
    "        return caminho_absoluto\n",
    "\n",
    "# Exemplo de uso\n",
    "pasta_rostos = criar_pasta_para_rostos()\n",
    "\n",
    "def ajustar_limites_rosto(box, width, height):\n",
    "   \n",
    "    x1, y1, x2, y2 = box\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(width, x2), min(height, y2)\n",
    "    return x1, y1, x2, y2\n",
    "def rastrear_ou_criar_id(box, face_trackers, proximo_id):\n",
    "  \n",
    "    for existing_id, tracker_info in face_trackers.items():\n",
    "        tracked_box, _ = tracker_info\n",
    "        iou = sobreposicao(box, tracked_box)\n",
    "        if iou > 0.5:\n",
    "            return existing_id, proximo_id, face_trackers\n",
    "    \n",
    "    face_id = proximo_id\n",
    "    proximo_id += 1\n",
    "    return face_id, proximo_id, face_trackers\n",
    "#face_roi, face_id, Rastreados, diretorio_saida\n",
    "\n",
    "def salvar_rosto(face_roi, face_id, Rastreados, diretorio_saida):\n",
    "\n",
    "    if face_id not in Rastreados:\n",
    "        Rastreados.add(face_id)\n",
    "        face_image_path = os.path.join(diretorio_saida, f\"face_{face_id}.jpg\")\n",
    "        cv2.imwrite(face_image_path, cv2.cvtColor(face_roi, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def sobreposicao(boxA, boxB):\n",
    "    # Calcular a interseção sobre união (IOU) entre dois retângulos\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "# Caminho para o arquivo de vídeo na mesma pasta do script\n",
    "script_dir = os.getcwd()\n",
    "input_video_path = os.path.join(script_dir, 'Video_tc.mp4')\n",
    "output_video_path = os.path.join(script_dir, 'output_video_11.mp4')  # Nome do vídeo de saída\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Chamar a função para detectar emoções no vídeo e salvar o vídeo processado\n",
    "detect_emotions(input_video_path, output_video_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
